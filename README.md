# DQN-Deep-RL
深度强化学习实战例子：DQN实现控制倒立摆
## DQN算法内容
主要思想：**用神经网络近似最优动作价值函数**$Q^*(s,a)$。该神经网络也称为深度Q网络(DQN)。

训练算法：**时间差分算法(TD)**。

**最优动作价值函数用最大化消除策略**：
$$Q_*(s_t,a_t)=\max_πQ_π(s_t,a_t)$$
当得知了最优动作价值函数后，可以使用其进行控制，**选择价值大的动作进行执行**。

神经网络结构：输入层为状态向量，输出层为动作个数的向量，每个元素代表对应动作的Q值。

### 训练流程
1. 将一条轨迹划分成n个$(s_t,a_t,r_t,s_{t+1})$四元组，存入**经验回放数组**。
2. 随机从经验回放数组中取出一个四元组，对DQN正向传播，得到$Q$值：
   
   $$q_j=Q(s_j,a_j;w_{now})$$

   $$q_{j+1}=\max_aQ(s_{j+1},a;w_{now})$$
4. 计算TD目标和TD误差：

   $$y_j=r_j+\gamma \cdot q_{j+1}$$
   
   $$\delta_j=q_j-y_j$$
6. 对DQN反向传播，得到梯度：
  
   $$g_j=\nabla_wQ(s_j,a_j;w_{now})$$
9. 做梯度下降更新DQN参数：

   $$w_{new}=w_{now}-\alpha \cdot \delta_j \cdot g_j$$

## 实现流程
实现内容：
- **经验回放数组**；
- **神经网络**，用来近似最优动作价值函数。
- **智能体**，即DQN算法；
